{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = os.getenv('SPARK_HOME')\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = \"python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self.spark = SparkSession.builder.master('local[*]').appName('Mediacal_cost_prediction').getOrCreate()\n",
    "\n",
    "    def load_data(self) -> DataFrame:\n",
    "        df = self.spark.read.csv(self.file_path, header=True, inferSchema=True)\n",
    "        return df\n",
    "\n",
    "    def remove_outliers(self, df: DataFrame, outliers_dict: dict) -> DataFrame:\n",
    "        conditions = [\n",
    "            (col(col_name) >= min_val) & (col(col_name) <= max_val)\n",
    "            for col_name, (min_val, max_val) in outliers_dict.items()\n",
    "        ]\n",
    "        combined_condition = conditions[0]\n",
    "        for condition in conditions[1:]:\n",
    "            combined_condition &= condition\n",
    "        df = df.filter(combined_condition)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, randn, when, lit, rand\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, StandardScaler, VectorAssembler\n",
    "from typing import List\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \n",
    "    def __init__(self, df: DataFrame):\n",
    "        self.df = df\n",
    "        \n",
    "    def string_indexer(self, input_cols: List[str], output_cols: List[str]) ->  \"FeatureEngineer\":\n",
    "        string_indexer = StringIndexer(inputCols=input_cols, outputCols=output_cols)\n",
    "        string_indexer = string_indexer.fit(self.df)\n",
    "        self.df = string_indexer.transform(self.df)\n",
    "        return self\n",
    "        \n",
    "    def One_hot_encoder(self, input_col: List[str], output_col: List[str]) ->  \"FeatureEngineer\": \n",
    "        one_hot_encoder = OneHotEncoder(inputCols=input_col, outputCols=output_col)\n",
    "        one_hot_encoder = one_hot_encoder.fit(self.df)\n",
    "        self.df = one_hot_encoder.transform(self.df)\n",
    "        return self\n",
    "        \n",
    "    def convert_to_binary(self, cols_to_binary: List[str]) -> \"FeatureEngineer\":\n",
    "        for column in cols_to_binary:\n",
    "            self.df = self.df.withColumn(column, col(column).cast('int'))\n",
    "        return self\n",
    "    \n",
    "    def charges_to_flaot(self):\n",
    "        self.df = self.df.withColumn('charges', col('charges').cast('double'))\n",
    "        return self\n",
    "    \n",
    "    def assemble_numerical_features(self, cols: List[str], output_col_name: str) ->  \"FeatureEngineer\":\n",
    "        assembler = VectorAssembler(inputCols=cols, outputCol=output_col_name)\n",
    "        self.df = assembler.transform(self.df)\n",
    "        return self\n",
    "    \n",
    "    def normalize_fetures(self, numerical_col_vector: str, output_name: str) -> \"FeatureEngineer\":\n",
    "        scaler = StandardScaler(inputCol=numerical_col_vector, outputCol=output_name,\n",
    "                                withStd=True, withMean=True)\n",
    "        scaler = scaler.fit(self.df)\n",
    "        self.df = scaler.transform(self.df)\n",
    "        return self\n",
    "    \n",
    "        \n",
    "    def get_DataFrame(self) -> DataFrame:\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "class ModelHandler:\n",
    "    \n",
    "    def __init__(self, train, test):\n",
    "        self.train = train\n",
    "        self.test = test        \n",
    "        \n",
    "    def train_model(self, regressor):\n",
    "        reg = regressor.fit(self.train)\n",
    "        return reg\n",
    "\n",
    "    def convert_to_rdd(self, model):\n",
    "        def transform_and_convert(df):\n",
    "            pred_df = model.transform(df).select('prediction', 'charges').dropna()\n",
    "            return pred_df.rdd.map(tuple)\n",
    "        \n",
    "        pred_train_rdd = transform_and_convert(self.train)\n",
    "        pred_test_rdd = transform_and_convert(self.test)\n",
    "        return pred_train_rdd, pred_test_rdd\n",
    "\n",
    "    \n",
    "    def evaluate_model(self, model_name, rdd_train_data, rdd_test_data) -> None:\n",
    "        \n",
    "        metrics_train = RegressionMetrics(rdd_train_data)\n",
    "        metrics_test = RegressionMetrics(rdd_test_data)\n",
    "        \n",
    "        print(f\"\\nModel name: {model_name}\")\n",
    "        print(\"Training Data Metrics:\")\n",
    "        print(f\"  MSE: {metrics_train.meanSquaredError}\")\n",
    "        print(f\"  RMSE: {metrics_train.rootMeanSquaredError}\")\n",
    "        print(f\"  MAE: {metrics_train.meanAbsoluteError}\")\n",
    "        print(f\"  R2: {metrics_train.r2}\")\n",
    "\n",
    "        print(\"\\nTesting Data Metrics:\")\n",
    "        print(f\"  MSE: {metrics_test.meanSquaredError}\")\n",
    "        print(f\"  RMSE: {metrics_test.rootMeanSquaredError}\")\n",
    "        print(f\"  MAE: {metrics_test.meanAbsoluteError}\")\n",
    "        print(f\"  R2: {metrics_test.r2}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "class PipelineManager:\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        \n",
    "        \n",
    "    def process_data(self, outliers_dict: dict,\n",
    "                     string_indexer_cols: List[str],\n",
    "                     string_indexer_output: str, \n",
    "                     cols_to_binary: List[str], \n",
    "                     cols_for_one_hot: List[str],\n",
    "                     output_for_one_hot: List[str],\n",
    "                     assemble_numerical_cols: List[str],\n",
    "                     assmble_numerical_output_name: str,\n",
    "                     cols_to_normalize: List[str],\n",
    "                     normalized_output: str, \n",
    "                     final_cols: List[str],\n",
    "                     final_output: str):\n",
    "        \n",
    "        data_loader = DataLoader(self.data_path)\n",
    "        df = data_loader.load_data()\n",
    "        df = data_loader.remove_outliers(df, outliers_dict)\n",
    "            \n",
    "        features_engineer = FeatureEngineer(df)        \n",
    "        features_engineer.string_indexer(input_cols=string_indexer_cols, output_cols=string_indexer_output)\n",
    "        features_engineer.convert_to_binary(cols_to_binary=cols_to_binary)\n",
    "        features_engineer.charges_to_flaot()\n",
    "        features_engineer.One_hot_encoder(input_col=cols_for_one_hot, output_col=output_for_one_hot)\n",
    "        features_engineer.assemble_numerical_features(cols=assemble_numerical_cols, output_col_name=assmble_numerical_output_name)\n",
    "        features_engineer.normalize_fetures(numerical_col_vector=cols_to_normalize, output_name=normalized_output)\n",
    "        features_engineer.assemble_numerical_features(cols=final_cols, output_col_name=final_output)\n",
    "        df = features_engineer.get_DataFrame()\n",
    "        return df             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model name: GBT Regressor\n",
      "Training Data Metrics:\n",
      "  MSE: 10145338.263716992\n",
      "  RMSE: 3185.1747618799495\n",
      "  MAE: 1827.1931553927914\n",
      "  R2: 0.8282466917987635\n",
      "\n",
      "Testing Data Metrics:\n",
      "  MSE: 13439840.613933718\n",
      "  RMSE: 3666.038817843275\n",
      "  MAE: 2183.592502942301\n",
      "  R2: 0.7021241270836316\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = os.getenv('SPARK_HOME')\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'python'\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"] = 'python'\n",
    "os.environ['PYSPARK_PYTHON'] = \"python\"\n",
    "\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    pipeline_manager = PipelineManager(\"../data/medical.csv\")\n",
    "    df = pipeline_manager.process_data(outliers_dict={\"charges\": (0, 35000), \"bmi\": (0, 45)},\n",
    "                                  string_indexer_cols=[\"sex\", \"smoker\", \"region\"], string_indexer_output=[\"sex_index\", \"smoker_index\", \"region_index\"],\n",
    "                                  cols_to_binary=[\"sex_index\", \"smoker_index\"],\n",
    "                                  cols_for_one_hot=['region_index'], output_for_one_hot=[\"region_one_hot\"],\n",
    "                                  assemble_numerical_cols=[\"bmi\", \"children\", \"age\"], assmble_numerical_output_name='numerical_cols_vector',\n",
    "                                  cols_to_normalize='numerical_cols_vector', normalized_output=\"scaled_numerical_cols_vector\",\n",
    "                                  final_cols=['sex_index', 'smoker_index', 'region_one_hot','scaled_numerical_cols_vector'], final_output='final_features_vector')   \n",
    "    \n",
    "    train, test = df.randomSplit([0.75, 0.25])\n",
    "    \n",
    "    model_handler = ModelHandler(train, test)\n",
    "    regressor = model_handler.train_model(regressor=GBTRegressor(featuresCol='final_features_vector', labelCol='charges'))\n",
    "    pred_train_rdd, pred_test_rdd = model_handler.convert_to_rdd(regressor)\n",
    "    model_handler.evaluate_model(\"GBT Regressor\", pred_train_rdd, pred_test_rdd)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
